{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlew/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of the ciks we're interested in\n",
    "f = open(\"final_ciks.txt\", \"r\")\n",
    "ciks = f.read().strip('[]').split(', ')\n",
    "for cik in ciks:\n",
    "    cik = str(cik).strip(\"'\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'1209028'\",\n",
       " \"'1069183'\",\n",
       " \"'825313'\",\n",
       " \"'1144980'\",\n",
       " \"'1800'\",\n",
       " \"'935036'\",\n",
       " \"'1113232'\",\n",
       " \"'796343'\",\n",
       " \"'7084'\",\n",
       " \"'1101215'\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ciks[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a258b72b744f4203b76d881ea9160da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=303.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing contents of 10ks/1209028/rawtext :\n",
      "\t1209028_2011-02-11.txt\n",
      "\t1209028_2012-02-23.txt\n",
      "\t...\n",
      "\tcreated: 10ks/1209028/merged_texts.txt\n",
      "Processing contents of 10ks/1069183/rawtext :\n",
      "\t1069183_2017-03-06.txt\n",
      "\t1069183_2011-03-14.txt\n",
      "\t...\n",
      "\tcreated: 10ks/1069183/merged_texts.txt\n",
      "Processing contents of 10ks/825313/rawtext :\n",
      "\t825313_2016-02-11.txt\n",
      "\t825313_2003-03-27.txt\n",
      "\t...\n",
      "\tcreated: 10ks/825313/merged_texts.txt\n",
      "Processing contents of 10ks/1144980/rawtext :\n",
      "\t1144980_2011-02-28.txt\n",
      "\t1144980_2008-02-29.txt\n",
      "\t...\n",
      "\tcreated: 10ks/1144980/merged_texts.txt\n",
      "Processing contents of 10ks/1800/rawtext :\n",
      "\t1800_2008-02-19.txt\n",
      "\t1800_2011-02-18.txt\n",
      "\t...\n",
      "\tcreated: 10ks/1800/merged_texts.txt\n",
      "Processing contents of 10ks/935036/rawtext :\n",
      "\t935036_2000-12-29.txt\n",
      "\t935036_2003-12-23.txt\n",
      "\t...\n",
      "\tcreated: 10ks/935036/merged_texts.txt\n",
      "Processing contents of 10ks/1113232/rawtext :\n",
      "\t1113232_2010-03-15.txt\n",
      "\t1113232_2002-03-12.txt\n",
      "\t...\n",
      "\tcreated: 10ks/1113232/merged_texts.txt\n",
      "Processing contents of 10ks/796343/rawtext :\n",
      "\t796343_2003-02-26.txt\n",
      "\t796343_2015-01-20.txt\n",
      "\t...\n",
      "\tcreated: 10ks/796343/merged_texts.txt\n",
      "Processing contents of 10ks/7084/rawtext :\n",
      "\t7084_2007-08-27.txt\n",
      "\t7084_2017-02-17.txt\n",
      "\t...\n",
      "\tcreated: 10ks/7084/merged_texts.txt\n",
      "Processing contents of 10ks/1101215/rawtext :\n",
      "\t1101215_2005-03-04.txt\n",
      "\t1101215_2013-02-28.txt\n",
      "\t...\n",
      "\tcreated: 10ks/1101215/merged_texts.txt\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# now to create an aggregate text body...\n",
    "for ind, cik in enumerate(tqdm(ciks)):\n",
    "    path = '10ks/' + cik.strip(\"'\") + '/rawtext'\n",
    "    if ind < 10:\n",
    "        print('Processing contents of ' + path + ' :')\n",
    "    f_list = os.listdir(path)\n",
    "    merged_texts = []\n",
    "    for entry, f_name in enumerate(f_list):\n",
    "        if entry < 2 and ind < 5:\n",
    "            print('\\t' + str(f_name))\n",
    "        f = open(path + '/' + f_name, \"r\")\n",
    "        f = f.read()\n",
    "        #print(f'length of doc: {len(f)}')\n",
    "        merged_texts.append(f)\n",
    "        #print(f'length of merged_doc: {len(merged_texts)}')\n",
    "    if ind < 5:\n",
    "        print('\\t...')\n",
    "    path = '10ks/' + cik.strip(\"'\")\n",
    "    if ind < 5:\n",
    "        print('\\tcreated: ' + path + \"/merged_texts.p\")\n",
    "    pickle.dump(merged_texts, open(path + '/merged_texts.p', \"wb\" ) )\n",
    "#     merged_doc_file = open(path + \"/merged_texts.txt\",\"w\")\n",
    "#     merged_doc_file.write(merged_texts)\n",
    "#     merged_doc_file.close()\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the workflow on a small subset of the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af4204b6b3b4dc1a451f0b3a68c3233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "newtext = []\n",
    "for entry in tqdm(merged_texts):\n",
    "    # getting rid of the terms seperately uses less memory\n",
    "    entry = entry.replace('Form 10-K','')\n",
    "    entry = entry.replace('FORM 10-K','')\n",
    "    entry = entry.replace('10-K','')\n",
    "    entry = re.sub(r'\\d+', '', entry)#remove numbers\n",
    "    entry = \"\".join([char.lower() for char in entry if char not in string.punctuation])\n",
    "    #^^remove punctuation and make lowercase\n",
    "    entry = \"\".join([char for char in entry if not char.isdigit()])\n",
    "    #^^remove numbers\n",
    "    entry = \"\".join([char for char in entry if not char.isdigit()])\n",
    "    #^^remove numbers\n",
    "    entry = entry.replace('million','')\n",
    "    entry = re.sub('\\s+', ' ', entry).strip() #remove doublespaces\n",
    "    newtext.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaarated</th>\n",
       "      <th>aaarated</th>\n",
       "      <th>aarated</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbreviationsaclallowance</th>\n",
       "      <th>abbreviationsascaccounting</th>\n",
       "      <th>abcp</th>\n",
       "      <th>abetted</th>\n",
       "      <th>...</th>\n",
       "      <th>zion</th>\n",
       "      <th>zionk</th>\n",
       "      <th>zions</th>\n",
       "      <th>zionsbank</th>\n",
       "      <th>zionw</th>\n",
       "      <th>zionxkhtm</th>\n",
       "      <th>zionz</th>\n",
       "      <th>zip</th>\n",
       "      <th>zmfu</th>\n",
       "      <th>zmsc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaaaarated  aaarated  aarated  ab  abbreviationsaclallowance  \\\n",
       "0   0    0           0         0        0   0                          1   \n",
       "1   0    0           0         0        0   0                          1   \n",
       "2  11    8           0        10        1   0                          0   \n",
       "3   0    0           0         0        0   0                          1   \n",
       "4   2    1           0         4        0   0                          0   \n",
       "\n",
       "   abbreviationsascaccounting  abcp  abetted  ...  zion  zionk  zions  \\\n",
       "0                           0     0        0  ...     1      0    122   \n",
       "1                           0     0        0  ...     1      0    127   \n",
       "2                           0     1        0  ...     1      0    230   \n",
       "3                           0     0        1  ...     1      0    138   \n",
       "4                           0     0        0  ...     1      0    203   \n",
       "\n",
       "   zionsbank  zionw  zionxkhtm  zionz  zip  zmfu  zmsc  \n",
       "0          0      0          1      0    1     0     0  \n",
       "1          0      0          1      0    1     0     1  \n",
       "2          0      0          0      0    1     0     7  \n",
       "3          0      4          1      3    1     0     0  \n",
       "4          0      0          0      0    1     0     0  \n",
       "\n",
       "[5 rows x 9558 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "months = ['january','february','march','april','june','july','august','september','october','november','december']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(months + ['company', 'financial'])\n",
    "vect=CountVectorizer(ngram_range=(1,1),stop_words=stop_words)\n",
    "fin=vect.fit_transform(newtext)\n",
    "#merged texts is the most recent one from the above generate files\n",
    "pd.DataFrame(fin.toarray(),columns=vect.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98117495e-01, 1.22809981e-03, 5.50534695e-06, 6.43277716e-04,\n",
       "        5.62185245e-06],\n",
       "       [9.96979596e-01, 2.75999102e-03, 5.69040728e-06, 2.48911186e-04,\n",
       "        5.81182077e-06],\n",
       "       [6.16682087e-06, 3.72186013e-05, 6.01336270e-06, 9.99944478e-01,\n",
       "        6.12336050e-06],\n",
       "       [9.97533436e-01, 1.48757181e-03, 5.59244593e-06, 9.67685094e-04,\n",
       "        5.71419809e-06],\n",
       "       [6.93913492e-06, 6.94507288e-06, 6.76274153e-06, 9.99972483e-01,\n",
       "        6.87049130e-06],\n",
       "       [8.51298712e-01, 7.13314669e-05, 6.98082674e-05, 1.48489067e-01,\n",
       "        7.10806712e-05],\n",
       "       [9.60359440e-01, 3.87665756e-02, 5.84160268e-06, 8.62172582e-04,\n",
       "        5.96979628e-06],\n",
       "       [4.73148644e-01, 5.26834882e-01, 5.39708229e-06, 5.57523393e-06,\n",
       "        5.50161704e-06],\n",
       "       [9.98544789e-01, 7.63992186e-05, 7.47990476e-05, 1.22835569e-03,\n",
       "        7.56568744e-05],\n",
       "       [6.87625392e-01, 3.11806496e-01, 5.47949544e-06, 5.57040540e-04,\n",
       "        5.59177291e-06],\n",
       "       [6.48152181e-01, 6.38036210e-05, 6.23863549e-05, 3.51658553e-01,\n",
       "        6.30753712e-05],\n",
       "       [7.36062189e-06, 8.78640332e-02, 4.90388186e-06, 8.51529132e-01,\n",
       "        6.05945704e-02],\n",
       "       [2.46128634e-01, 7.53754775e-01, 5.36186701e-06, 1.05765321e-04,\n",
       "        5.46341277e-06],\n",
       "       [7.42678431e-01, 8.10227106e-05, 7.92354319e-05, 2.57081159e-01,\n",
       "        8.01517587e-05],\n",
       "       [1.23304125e-04, 8.82246741e-01, 5.47355630e-06, 1.17618878e-01,\n",
       "        5.60364898e-06],\n",
       "       [1.07213367e-03, 9.73137115e-01, 4.98719617e-06, 2.57806712e-02,\n",
       "        5.09249655e-06],\n",
       "       [5.80372229e-05, 4.08385712e-01, 4.53415521e-06, 4.73605649e-01,\n",
       "        1.17946068e-01],\n",
       "       [7.53900389e-01, 7.92527952e-05, 7.74913443e-05, 2.45864341e-01,\n",
       "        7.85251375e-05]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda=LatentDirichletAllocation(n_components=5)\n",
    "da_lda=lda.fit_transform(fin)\n",
    "da_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "capital       securities    availablefor  securities    securities    \n",
      "bank          loans         excerpts      net           net           \n",
      "loans         stock         resultsof     income        losses        \n",
      "securities    value         andinterest   loans         loans         \n",
      "loan          net           andfinancial  value         fair          \n",
      "value         income        analytic      loan          loss          \n",
      "risk          fair          staffs        bank          value         \n",
      "stock         rate          question      zions         assets        \n",
      "credit        loan          think         assets        total         \n",
      "income        credit        asked         credit        loan          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting=np.argsort(lda.components_)[:,::-1]\n",
    "features=np.array(vect.get_feature_names())\n",
    "import mglearn\n",
    "mglearn.tools.print_topics(topics=range(5), feature_names=features,sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the aggregate list of documents to do topic modelling on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9af276086341538aeec6b147cbec36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=303.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the number of documents is 5619\n"
     ]
    }
   ],
   "source": [
    "newtext = []\n",
    "for ind, cik in enumerate(tqdm(ciks)):\n",
    "    path = '10ks/' + cik.strip(\"'\") + '/merged_texts.p'\n",
    "    cik_docs = pickle.load(open(path,\"rb\"))\n",
    "    for entry in cik_docs:\n",
    "        # getting rid of the terms seperately uses less memory\n",
    "        entry = entry.replace('Form 10-K','')\n",
    "        entry = entry.replace('FORM 10-K','')\n",
    "        entry = entry.replace('10-K','')\n",
    "        entry = re.sub(r'\\d+', '', entry)#remove numbers\n",
    "        entry = \"\".join([char.lower() for char in entry if char not in string.punctuation])\n",
    "        #^^remove punctuation and make lowercase\n",
    "        entry = \"\".join([char for char in entry if not char.isdigit()])\n",
    "        #^^remove numbers\n",
    "        entry = \"\".join([char for char in entry if not char.isdigit()])\n",
    "        #^^remove numbers\n",
    "        entry = entry.replace('million','')\n",
    "        entry = re.sub('\\s+', ' ', entry).strip() #remove doublespaces\n",
    "        newtext.append(entry)\n",
    "print(f'the number of documents is {len(newtext)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apples and oranges are similar. Boots and hippos aren't.\n"
     ]
    }
   ],
   "source": [
    "print(u\"Apples and oranges are similar. Boots and hippos aren't.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8f7ded4ae64914b59e592025d66e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5619.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1015812 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-a1af60335365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlemma_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#for word in nlp(doc.split()):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlemma_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlemma_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    439\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1015812 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en')\n",
    "lemma_docs = []\n",
    "\n",
    "for doc in tqdm(newtext):\n",
    "    lemma_text = []\n",
    "    #for word in nlp(doc.split()):\n",
    "    for word in nlp(doc):\n",
    "        lemma_text.append(word.lemma_)\n",
    "    lemma_docs.append(' '.join(lemma_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>aaaaaarated</th>\n",
       "      <th>aaaaamortgagebacked</th>\n",
       "      <th>aaaaaobligations</th>\n",
       "      <th>aaaaaprerefunded</th>\n",
       "      <th>aaaaarated</th>\n",
       "      <th>...</th>\n",
       "      <th>μgl</th>\n",
       "      <th>μl</th>\n",
       "      <th>μm</th>\n",
       "      <th>μscm</th>\n",
       "      <th>上a</th>\n",
       "      <th>公o</th>\n",
       "      <th>司i</th>\n",
       "      <th>技z术o</th>\n",
       "      <th>有l限a</th>\n",
       "      <th>海c斯z丹o赛u</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 268559 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaaa  aaaaa  aaaaaa  aaaaaarated  aaaaamortgagebacked  \\\n",
       "0   0    6     0      0       0            0                    0   \n",
       "1   2    7     0      0       0            0                    0   \n",
       "2   0    7     0      1       0            0                    0   \n",
       "3   0    2     0      1       0            0                    0   \n",
       "4   0    1     0      0       0            0                    0   \n",
       "\n",
       "   aaaaaobligations  aaaaaprerefunded  aaaaarated  ...  μgl  μl  μm  μscm  上a  \\\n",
       "0                 0                 0           0  ...    0   0   0     0   0   \n",
       "1                 0                 0           0  ...    0   0   0     0   0   \n",
       "2                 0                 0           0  ...    0   0   0     0   0   \n",
       "3                 0                 0           0  ...    0   0   0     0   0   \n",
       "4                 0                 0           0  ...    0   0   0     0   0   \n",
       "\n",
       "   公o  司i  技z术o  有l限a  海c斯z丹o赛u  \n",
       "0   0   0     0     0         0  \n",
       "1   0   0     0     0         0  \n",
       "2   0   0     0     0         0  \n",
       "3   0   0     0     0         0  \n",
       "4   0   0     0     0         0  \n",
       "\n",
       "[5 rows x 268559 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months = ['january','february','march','april','june','july','august','september','october','november','december']\n",
    "freq_words = ['operating','product','products','business','value','fair','statements','assets','operations','including','tax','consolidated','company', 'financial', 'stock','cash','net','year', 'years','market','income','certain','related','costs']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(months + freq_words)\n",
    "vect=CountVectorizer(ngram_range=(1,1),stop_words=stop_words)\n",
    "fin=vect.fit_transform(newtext)\n",
    "#merged texts is the most recent one from the above generate files\n",
    "pd.DataFrame(fin.toarray(),columns=vect.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01199838e-01, 6.70255610e-06, 8.97449354e-01, ...,\n",
       "        6.70347905e-06, 6.70305811e-06, 1.31729366e-03],\n",
       "       [8.73662711e-02, 5.54766558e-06, 8.87730861e-01, ...,\n",
       "        5.54794995e-06, 5.54788537e-06, 2.48751289e-02],\n",
       "       [9.26630629e-02, 3.48252718e-06, 7.97850576e-01, ...,\n",
       "        3.48241997e-06, 3.48218225e-06, 1.09468949e-01],\n",
       "       ...,\n",
       "       [3.41499665e-06, 2.25864869e-04, 9.98685543e-01, ...,\n",
       "        3.41308068e-06, 3.41291187e-06, 3.41312090e-06],\n",
       "       [2.23621367e-03, 7.57057235e-04, 9.93620352e-01, ...,\n",
       "        3.11389427e-06, 1.24793326e-03, 3.11428061e-06],\n",
       "       [5.11462498e-05, 5.11402366e-05, 8.11735197e-01, ...,\n",
       "        5.11501992e-05, 1.87957919e-01, 5.11515979e-05]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda=LatentDirichletAllocation(n_components=8)\n",
    "da_lda=lda.fit_transform(fin)\n",
    "da_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       topic 5       topic 6       topic 7       \n",
      "--------      --------      --------      --------      --------      --------      --------      --------      \n",
      "properties    energy        loans         development   gas           sales         fiscal        services      \n",
      "property      gas           loan          agreement     oil           plan          sales         revenue       \n",
      "common        electric      securities    common        production    ended         plan          fiscal        \n",
      "ended         power         bank          clinical      natural       agreement     incorporated  ended         \n",
      "real          ppl           credit        research      reserves      rate          credit        customers     \n",
      "debt          rate          capital       sales         health        credit        stores        sales         \n",
      "shares        plan          losses        shares        future        notes         ended         common        \n",
      "estate        new           rate          future        properties    based         report        results       \n",
      "rate          cost          total         ended         table         results       reference     shares        \n",
      "partnership   customers     management    fda           price         loss          item          based         \n",
      "management    regulatory    risk          regulatory    ended         new           agreement     management    \n",
      "agreement     approximately investment    additional    care          expense       new           plan          \n",
      "lease         credit        table         securities    approximately accounting    common        future        \n",
      "notes         edison        loss          patent        prices        cost          approximately information   \n",
      "share         rates         insurance     results       total         report        results       table         \n",
      "preferred     ended         based         loss          plan          services      information   accounting    \n",
      "approximately agreement     changes       approval      based         debt          based         loss          \n",
      "capital       natural       equity        drug          percent       table         share         new           \n",
      "credit        capital       ended         revenue       accounting    total         management    compensation  \n",
      "table         fuel          federal       rights        common        service       accounting    securities    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting=np.argsort(lda.components_)[:,::-1]\n",
    "features=np.array(vect.get_feature_names())\n",
    "import mglearn\n",
    "mglearn.tools.print_topics(topics=range(8), feature_names=features,sorting=sorting, topics_per_chunk=8, n_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved Cell(s) for Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91840df46624f9f950a586548b93651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=303.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now the final doc\n",
    "merged_text = []\n",
    "for ind, cik in enumerate(tqdm(ciks)):\n",
    "    f_path = '10ks/' + cik.strip(\"'\") + '/merged_texts.p'\n",
    "    f = open(f_path, \"r\")\n",
    "    f = f.read()\n",
    "    merged_text.append(f)\n",
    "complete_doc_file = open(\"all_texts.txt\",\"w\") \n",
    "complete_doc_file.write(str(merged_texts))\n",
    "complete_doc_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
